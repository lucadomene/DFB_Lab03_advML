{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0499f7-d898-4f85-8528-dc3850a74439",
   "metadata": {},
   "source": [
    "# Laboratory on Adversarial Machine Learning\n",
    "_Digital Forensics and Biometrics_ A.A. 2025/2026\n",
    "\n",
    "Lecturer: prof. **Simone Milani** (simone.milani@dei.unipd.it)\n",
    "\n",
    "Teaching Assistants: **Mattia Tamiazzo** (mattia.tamiazzo.1@phd.unipd.it); **Luca Domeneghetti** (luca.domeneghetti@studenti.unipd.it)\n",
    "\n",
    "_(Sources: [TensorFlow approach](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb) and [PyTorch approach](https://docs.pytorch.org/tutorials/beginner/fgsm_tutorial.html))_\n",
    "\n",
    "### Prerequisites\n",
    "This laboratory requires some basic prior knowledge on _machine learning_ and _neural networks_.\n",
    "\n",
    "The additional information required to understand the laboratory will be provided during the lecture.\n",
    "\n",
    "The following aspects are assumed to be known:\n",
    "- What is a ML model and how to interact with it\n",
    "- Tensors\n",
    "- Gradients and loss functions (in particular Cross Entropy and Log Softmax)\n",
    "- Backward propagation in a Neural Network\n",
    "\n",
    "### Contents\n",
    "The goal of this laboratory is to create an adversarial data sample (i.e. an image) that is capable of fooling a ML algorithm so that it mislabels the input sample. To do so, we will use a **Fast Gradient Sign Method** (FGSM) to shift our input sample towards a desired target class.\n",
    "\n",
    "By the end of the laboratory the student will have acquired the following skills:\n",
    "- Basic usage of Pytorch framework\n",
    "- Load and use a pretrained network in evaluation mode\n",
    "- Compute the signed gradient through a backpropagation\n",
    "- **Create an adversarial data sample**\n",
    "- **Shift the input prediction towards a desired target**\n",
    "___\n",
    "# Threat Model\n",
    "\n",
    "For context, there are many categories of adversarial attacks, each with\n",
    "a different goal and assumption of the attacker's knowledge. However, in\n",
    "general the overarching goal is to add the least amount of perturbation\n",
    "to the input data to cause the desired misclassification. There are\n",
    "several kinds of assumptions of the attacker's knowledge, two of which\n",
    "are: **white-box** and **black-box**. A *white-box* attack assumes the\n",
    "attacker has full knowledge and access to the model, including\n",
    "architecture, inputs, outputs, and weights. A *black-box* attack assumes\n",
    "the attacker only has access to the inputs and outputs of the model, and\n",
    "knows nothing about the underlying architecture or weights. There are\n",
    "also several types of goals, including **misclassification** and\n",
    "**source/target misclassification**. A goal of *misclassification* means\n",
    "the adversary only wants the output classification to be wrong but does\n",
    "not care what the new classification is. A *source/target\n",
    "misclassification* means the adversary wants to alter an image that is\n",
    "originally of a specific source class so that it is classified as a\n",
    "specific target class.\n",
    "\n",
    "In this case, the FGSM attack is a *white-box* attack with the goal of\n",
    "*misclassification* and *source/target misclassification*. With this background information, we can now\n",
    "discuss the attack in detail.\n",
    "\n",
    "# Fast Gradient Sign Method\n",
    "The fast gradient sign method works by using the gradients of the neural network to create an adversarial example. For an input image, the method uses the gradients of the loss with respect to the input image to create a new image that maximises the loss. This new image is called the adversarial image. This can be summarised using the following expression:\n",
    "$$\\text{adv}_x = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
    "\n",
    "where\n",
    "\n",
    "* adv_x : Adversarial image.\n",
    "* x : Original input image.\n",
    "* y : Original input label.\n",
    "* $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
    "* $\\theta$ : Model parameters.\n",
    "* $J$ : Loss.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/fgsm_panda_image.png)\n",
    "\n",
    "An intriguing property here, is the fact that the gradients are taken with respect to the input image. This is done because the objective is to create an image that maximises the loss. A method to accomplish this is to find how much each pixel in the image contributes to the loss value, and add a perturbation accordingly. This works pretty fast because it is easy to find how each input pixel contributes to the loss by using the chain rule and finding the required gradients. Hence, the gradients are taken with respect to the image. In addition, since the model is no longer being trained (thus the gradient is not taken with respect to the trainable variables, i.e., the model parameters), and so the model parameters remain constant. The only goal is to fool an already trained model.\n",
    "\n",
    "So let's try and fool a pretrained model. In this tutorial, the model used is [MobileNetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) pretrained on [ImageNet](http://www.image-net.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463bacb-7279-467f-bd15-a67e661a8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as Fv\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd6cde-f2c3-4397-a5af-53e4203cbf9a",
   "metadata": {},
   "source": [
    "## 1. Setting up the Target Model & Utilities\n",
    "\n",
    "In this laboratory, we will perform adversarial attacks against **MobileNetV2**, a lightweight Convolutional Neural Network (CNN) pre-trained on the **ImageNet** dataset.\n",
    "\n",
    "![](https://www.researchgate.net/publication/350152088/figure/fig1/AS:1002717703045121@1616077938892/The-proposed-MobileNetV2-network-architecture.png)\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "To prepare the environment for the attack, we need to perform three key steps:\n",
    "\n",
    "1.  **Load the Model:** We import MobileNetV2 with its default pre-trained weights.\n",
    "    * *Note:* We explicitly set the model to **evaluation mode** (`model.eval()`). This is critical because it locks the behavior of layers like `Dropout` and `BatchNorm`, ensuring consistent predictions during the attack generation.\n",
    "2.  **Define Preprocessing:** Standard ImageNet models require specific input normalization. We define a transformation pipeline that resizes images and normalizes them using the standard ImageNet mean ($\\mu$) and standard deviation ($\\sigma$).\n",
    "    $$\\text{Input}_{norm} = \\frac{\\text{Input} - \\mu}{\\sigma}$$\n",
    "3.  **Visualization Helpers:** Since the model requires normalized tensors, we cannot visualize them directly. We define two helper functions:\n",
    "    * `denormalize`: Reverses the math above to return the tensor to the original pixel space.\n",
    "    * `tensor2img`: Converts the PyTorch tensor (Channels, Height, Width) into a PIL Image (Height, Width, Channels) suitable for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3dd3a-1d0e-45eb-929c-2823da295318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Model\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model = mobilenet_v2(weights=weights)\n",
    "model.eval() # set to evaluation mode (batchnorm/dropout behavior changes)\n",
    "model.to(device)\n",
    "\n",
    "# define Transforms (Normalization constants)\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "# helper Functions\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Reverses the ImageNet normalization for visualization.\"\"\"\n",
    "    \n",
    "    # clone to avoid modifying the original tensor in place\n",
    "    tensor = tensor.clone().detach()\n",
    "    \n",
    "    mean = torch.tensor(MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(STD).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    return tensor * std + mean\n",
    "\n",
    "def tensor2img(tensor, denorm=True):\n",
    "    \"\"\"Converts a Tensor (normalized or not) to a PIL Image for plotting.\"\"\"\n",
    "    \n",
    "    # denormalize\n",
    "    denormalized = tensor.clone()\n",
    "    if denorm:\n",
    "        denormalized = denormalize(tensor)\n",
    "    \n",
    "    # clamp to [0, 1] range to avoid artifacts\n",
    "    clamped = torch.clamp(denormalized, 0, 1)\n",
    "    \n",
    "    # convert to Numpy (H, W, C)\n",
    "    img_numpy = clamped.squeeze(0).cpu().numpy()\n",
    "    img_numpy = np.transpose(img_numpy, (1, 2, 0))\n",
    "    \n",
    "    # convert to PIL (0-255)\n",
    "    return Image.fromarray((img_numpy * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cdcba-ba14-4bf3-8727-9b1420d8dd1b",
   "metadata": {},
   "source": [
    "### Exploring available classes\n",
    "\n",
    "To perform an adversarial attack, we first need a valid input sample. Since the model is trained on ImageNet, the input image must correspond to one of the 1,000 specific classes the model recognizes.\n",
    "\n",
    "In the following cell, we access the metadata attached to the model weights to list the available categories and identify a suitable subject for our test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b6866-7562-4a19-8b99-9c2fcbcc253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = weights.meta['categories']\n",
    "\n",
    "# print first 10 for brevity, or remove slice to see all\n",
    "print('\\n'.join(f'{i}\\t{element}' for i, element in enumerate(categories[:10])))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3d303-7a5e-4a54-8695-d3439e39083f",
   "metadata": {},
   "source": [
    "### Loading a sample image\n",
    "\n",
    "Next, we download an image to serve as the target for our adversarial attack.\n",
    "\n",
    "The code below retrieves a sample image (a Siamese cat) directly from a URL. You are free to replace this URL with an image of your choice from the web or from this [ImageNet sample repository](https://github.com/EliSchwartz/imagenet-sample-images).\n",
    "\n",
    "> **Note:** If you choose a custom image from GitHub, right-click the image and select \"Copy Image Link\" to ensure you use the **raw static URL** rather than the repository UI link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c0c18-614b-47b3-90f7-c7b59acce0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.purina.co.uk/sites/default/files/styles/square_medium_440x440/public/2022-06/Siamese%201.jpg'\n",
    "filename = 'siamese.jpeg'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    pil_image = Image.open(filename)\n",
    "    print(\"Image downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f9ee6-013c-4569-bc1a-84ce45469bea",
   "metadata": {},
   "source": [
    "With the image loaded, we must now format it for the model. We apply the preprocessing transforms defined earlier and use `unsqueeze(0)` to add a batch dimension, creating a tensor of shape $(1, C, H, W)$.\n",
    "\n",
    "We then pass this batch through the model to obtain the initial prediction and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165d6d9-e46e-4262-9566-e2c5297dbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the image ONCE and create the master tensor batch\n",
    "image_batch = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "# run inference\n",
    "output = model(image_batch)\n",
    "probs = F.softmax(output, dim=1)\n",
    "\n",
    "top_prob, top_catid = probs.max(1)\n",
    "label = categories[top_catid.item()]\n",
    "confidence = top_prob.item()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(f'{label}: {confidence*100:.2f}%')\n",
    "plt.imshow(tensor2img(image_batch))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058afd2-e385-46ef-b494-011b5df41e30",
   "metadata": {},
   "source": [
    "## 2. Constructing the Adversarial Perturbation\n",
    "\n",
    "We can now begin the attack on our model using the **Fast Gradient Sign Method (FGSM)**.\n",
    "\n",
    "The core idea is to create a perturbation that pushes the image across the decision boundary. To achieve this, we calculate the **gradient of the loss** with respect to the input image ($x$). This gradient, denoted as $\\nabla_x J(\\theta, x, y)$, indicates how we should adjust pixel values to maximize the classification error.\n",
    "\n",
    "The function below implements this logic:\n",
    "1.  It enables gradient tracking on the input tensor.\n",
    "2.  It computes the loss based on the model's current prediction.\n",
    "3.  It backpropagates to find the gradient and returns the **sign** of that gradient, which determines the direction of the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6abc2e-6cde-420e-8e6d-a0a00584770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(tensor, label_index):\n",
    "    \"\"\"\n",
    "    Calculates the gradient of the loss w.r.t the input tensor.\n",
    "    Note: We process the tensor directly, avoiding converting back and forth to Image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # clone the tensor to ensure we don't mess up the original computation graph\n",
    "    data = tensor.clone().detach().to(device)\n",
    "    data.requires_grad = True\n",
    "\n",
    "    # forward pass\n",
    "    output = model(data)\n",
    "    \n",
    "    # calculate Loss\n",
    "    target = torch.tensor([label_index]).to(device)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    \n",
    "    # backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # return the sign of the gradient\n",
    "    return data.grad.data.sign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1bd2d-1238-4400-9b43-6450125efd98",
   "metadata": {},
   "source": [
    "Let's test our `get_gradient()` function by running it against our chosen image and associated label's index.\n",
    "\n",
    "By plotting the perturbation we can visually see how the signed gradient displays in the RGB image domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4c860-0e40-4d62-98db-5e0b9bb14da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the current true label\n",
    "true_label_idx = categories.index('Siamese cat')\n",
    "\n",
    "# calculate gradient\n",
    "perturbation = get_gradient(image_batch, true_label_idx)\n",
    "\n",
    "# visualize the perturbation (noise)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Perturbation (Signed Gradient)\")\n",
    "plt.imshow(tensor2img(perturbation))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652bc8c-a958-4330-aff7-91eb53fbd940",
   "metadata": {},
   "source": [
    "### Measuring the \"noisiness\" of the image\n",
    "\n",
    "To evaluate the quality of our attack, we need to measure how much the adversarial image differs from the original. We will use the **Peak Signal-to-Noise Ratio (PSNR)**, a standard metric for image fidelity.\n",
    "\n",
    "The PSNR is calculated using the Mean Squared Error (MSE) between the two images:\n",
    "\n",
    "$$PSNR = 10 \\cdot \\log_{10}\\left(\\frac{MAX^2}{MSE}\\right)$$\n",
    "\n",
    "Where $MAX$ represents the maximum possible pixel value (1.0 for our floating-point tensors). A **higher PSNR** indicates less distortion, meaning the attack is harder for a human to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a7a00-7a80-4984-8118-0ec8250c2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img_tensor_1, img_tensor_2, max_val=1.0):\n",
    "    \"\"\"Calculates Peak Signal-to-Noise Ratio between two tensors.\"\"\"\n",
    "    \n",
    "    mse = torch.mean((img_tensor_1 - img_tensor_2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 10 * torch.log10((max_val ** 2) / mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7d1ba-01ec-411b-b101-46d68bab08ff",
   "metadata": {},
   "source": [
    "### Visualizing the attack\n",
    "\n",
    "To systematically evaluate the effectiveness of our adversarial examples, we define a dedicated display function. This utility performs three key tasks:\n",
    "\n",
    "1.  **Inference:** It passes the perturbed image back through the model to determine if the predicted label has changed (i.e., if the attack was successful).\n",
    "2.  **Quality Check:** It computes the PSNR metric to quantify the level of visual distortion introduced by the attack.\n",
    "3.  **Visualization:** It renders the final adversarial image with the new confidence scores and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea9f99-8cd5-44a1-aaca-517f941b60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attack_result(adv_tensor, orig_tensor, description, noise=None):\n",
    "    \"\"\"Displays the adversarial image, prediction, and PSNR.\"\"\"\n",
    "    \n",
    "    # get prediction for the adversarial image\n",
    "    output = model(adv_tensor)\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    top_prob, top_catid = probs.max(1)\n",
    "    label = categories[top_catid.item()]\n",
    "    confidence = top_prob.item()\n",
    "\n",
    "    # calculate metrics\n",
    "    psnr = calculate_psnr(adv_tensor, orig_tensor)\n",
    "    \n",
    "    # plot\n",
    "    if noise is not None:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axs[1].imshow(tensor2img(noise))\n",
    "        axs[1].set_title(\"Noise\")\n",
    "        axs[1].axis('off')\n",
    "        main_ax = axs[0]\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, figsize=(6, 6))\n",
    "        main_ax = ax\n",
    "\n",
    "    main_ax.imshow(tensor2img(adv_tensor))\n",
    "    main_ax.set_title(f'{description}\\nPred: {label} ({confidence*100:.2f}%)\\nPSNR: {psnr:.2f} dB')\n",
    "    main_ax.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6555b5-1f43-48da-9ccd-e86a821f2e63",
   "metadata": {},
   "source": [
    "## Task 1: Untargeted attack\n",
    "\n",
    "### Single Step Untargeted FSGM\n",
    "\n",
    "We are now ready to generate adversarial examples. We will iterate through a list of **epsilon ($\\epsilon$)** values, which control the magnitude of the perturbation.\n",
    "\n",
    "The core logic follows the **Fast Gradient Sign Method (FGSM)** update rule:\n",
    "\n",
    "$$x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the original image.\n",
    "* $\\epsilon$ (epsilon) is the attack strength.\n",
    "* $\\text{sign}(\\nabla_x J)$ is the direction of the gradient we calculated earlier.\n",
    "\n",
    "**The Trade-off:**\n",
    "* **Low $\\epsilon$:** The attack is subtle and harder to detect visually, but less likely to fool the model.\n",
    "* **High $\\epsilon$:** The attack is powerful and likely to succeed, but the noise becomes visible to the human eye, lowering the PSNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81ff84-1bd4-4b22-a1b2-7ec318ef7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.001, 0.01, 0.1, 0.15, 0.9]\n",
    "\n",
    "for eps in epsilons:\n",
    "    # untargeted attack: ADD epsilon * gradient to MAXIMIZE loss\n",
    "    adv_x = image_batch + eps * perturbation\n",
    "    \n",
    "    desc = f'Epsilon = {eps:.3f}' if eps else 'Input'\n",
    "    display_attack_result(adv_x, image_batch, desc, perturbation if eps else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17108a02-c40e-4106-9d4f-380d2268b640",
   "metadata": {},
   "source": [
    "### Iterative Untargeted FSGM\n",
    "\n",
    "While the standard FGSM takes a single, large step in the direction of the gradient, the **Iterative FGSM** takes multiple smaller steps.\n",
    "\n",
    "Instead of calculating the gradient once on the original image, we recalculate the gradient at every step based on the *current* perturbed image. This allows the attack to fine-tune the noise, often resulting in a successful misclassification with potentially less perceptible distortion.\n",
    "\n",
    "The update rule for each iteration $t$ is:\n",
    "\n",
    "$$x_{t+1} = x_t + \\alpha \\cdot \\text{sign}(\\nabla_{x_t} J(\\theta, x_t, y))$$\n",
    "\n",
    "Where:\n",
    "* $x_0$ is the original image.\n",
    "* $\\alpha$ (epsilon in the code below) is the step size for each iteration.\n",
    "* The gradient is re-computed at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ff858-47a9-4301-b811-3e3c24224ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# start from the original tensor\n",
    "adv_x = image_batch.clone()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # get gradient w.r.t current adversarial example\n",
    "    # Note: we pass the tensor directly; converting to image and back would lose precision.\n",
    "    iter_perturbation = get_gradient(adv_x, true_label_idx)\n",
    "    \n",
    "    # update image (gradient ascent for untargeted)\n",
    "    adv_x = adv_x + epsilon * iter_perturbation\n",
    "\n",
    "display_attack_result(adv_x, image_batch, f\"Iterative Attack ({epochs} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc4a64-b925-45ad-9267-27b37b03570a",
   "metadata": {},
   "source": [
    "## Task 2: Targeted Attack\n",
    "\n",
    "### Single Step Targeted FGSM\n",
    "\n",
    "In the previous task, we performed an **untargeted attack**. We simply wanted the model to make *any* mistake. By adding noise in the direction of the gradient, we pushed the image across the nearest decision boundary. This often results in a prediction that is semantically similar to the original (e.g., a Golden Retriever might be misclassified as a Labrador).\n",
    "\n",
    "However, a sophisticated attacker often wants to force a specific outcome (e.g., making a self-driving car classify a \"Stop\" sign as a \"Speed Limit 60\" sign). This is a **targeted attack**.\n",
    "\n",
    "#### Implementation Logic\n",
    "\n",
    "To achieve this, we modify the FGSM approach slightly:\n",
    "1.  **Target Selection:** We calculate the gradient of the input with respect to a specific **target label** ($y_{target}$) rather than the true label.\n",
    "2.  **Gradient Descent:** Instead of maximizing the loss (moving *away* from the true class), we want to **minimize** the loss with respect to our target class (moving *towards* it).\n",
    "\n",
    "Therefore, we **subtract** the perturbation instead of adding it:\n",
    "\n",
    "$$x_{adv} = x - \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y_{target}))$$\n",
    "\n",
    "You can find a list of all 1,000 ImageNet classes [here](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/). We encourage you to experiment with different target indices to see which classes are easier to mimic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f1626-2c11-4155-b4d7-d853e5a9fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = 200\n",
    "target_label_name = categories[target_idx]\n",
    "print(f'Targeting class {target_idx}: \"{target_label_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c0642-c477-4f5f-9d54-34ff16e1ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient relative to the TARGET class, not the true class.\n",
    "target_perturbation = get_gradient(image_batch, target_idx)\n",
    "\n",
    "epsilons = [0, 0.001, 0.01, 0.1, 0.15, 0.3, 0.6]\n",
    "\n",
    "for eps in epsilons:\n",
    "    # targeted attack: SUBTRACT epsilon * gradient to MINIMIZE loss w.r.t Target\n",
    "    # (gradient descent towards the target)\n",
    "    adv_x = image_batch - eps * target_perturbation\n",
    "    \n",
    "    desc = f'Targeted Eps = {eps:.3f}' if eps else 'Input'\n",
    "    display_attack_result(adv_x, image_batch, desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485d39b-7584-4e44-9f10-d9f72870ab8c",
   "metadata": {},
   "source": [
    "### Iterative Targeted FGSM\n",
    "\n",
    "When targeting a specific class that is semantically very different from the original image (e.g., turning a \"Cat\" into a \"Black Swan\"), a single gradient step is often insufficient. A large single step might overshoot the target manifold or push the image into a completely different, unintended class.\n",
    "\n",
    "To overcome this, we employ an **Iterative Targeted Attack**. Instead of one large jump, we take many small steps in the direction of the target class. This is essentially performing **Gradient Descent** on the image pixels to minimize the loss with respect to the target label.\n",
    "\n",
    "The update rule for each epoch $i$ becomes:\n",
    "\n",
    "$$x_{i+1} = x_i - \\alpha \\cdot \\text{sign}(\\nabla_{x_i} J(\\theta, x_i, y_{target}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3e717-9acb-47ee-81b2-c267273ed0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_target_idx = 100\n",
    "print(f'Hard Target {hard_target_idx}: \"{categories[hard_target_idx]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c89c2-e57f-4ac0-869f-e4d1cb3542fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001 # small step size\n",
    "epochs = 200\n",
    "epoch_reached = 0\n",
    "\n",
    "adv_x = image_batch.clone()\n",
    "\n",
    "for i in range(epochs):\n",
    "    # get gradient w.r.t target class based on current adversarial image\n",
    "    iter_grad = get_gradient(adv_x, hard_target_idx)\n",
    "    \n",
    "    # 2. update image (gradient descent towards target)\n",
    "    adv_x = adv_x - epsilon * iter_grad\n",
    "\n",
    "    current_pred = model(adv_x).argmax(dim=1).item()\n",
    "    if current_pred == hard_target_idx and not epoch_reached:\n",
    "        epoch_reached = i\n",
    "\n",
    "print(f\"Target reached at epoch {epoch_reached}!\")\n",
    "display_attack_result(adv_x, image_batch, f\"Targeted Iterative ({epochs} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d3d01-6c65-45c0-bf78-a31f85d57a21",
   "metadata": {},
   "source": [
    "## Task 3: Background vs. Foreground Sensitivity\n",
    "\n",
    "A critical question in understanding Convolutional Neural Networks is determining *where* the model looks. Does it classify a \"car\" because of the wheels and chassis (foreground), or because it detects a road and asphalt (background)?\n",
    "\n",
    "To investigate this, we can restrict our adversarial attack to specific regions of the image.\n",
    "\n",
    "### Methodology\n",
    "We will employ a **Masked Gradient Attack**. The process is as follows:\n",
    "\n",
    "1.  **Load a Mask:** We import a binary mask ($M$) where pixel values are $1$ for the region of interest (e.g., the cat) and $0$ elsewhere.\n",
    "2.  **Compute Gradient:** We calculate the gradient for the entire image as usual.\n",
    "3.  **Apply Mask:** Before updating the image, we perform an element-wise multiplication between the gradient and the mask.\n",
    "    $$\\text{perturbation} = \\nabla_x J(\\theta, x, y) \\odot M$$\n",
    "    * If $M_{ij} = 0$, the gradient becomes 0, and that pixel remains unchanged.\n",
    "    * If $M_{ij} = 1$, the perturbation is applied normally.\n",
    "\n",
    "By toggling the mask (using `1 - mask`), we can choose to attack **only the background** or **only the foreground** to see which region contributes most to the model's vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f1537-18b7-4899-8f0f-930dca7dce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = image_batch.shape[2]\n",
    "input_width = image_batch.shape[3]\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((input_height, input_width)),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "mask_raw = Image.open(\"mask.png\").convert('RGB')\n",
    "mask = mask_transform(mask_raw)\n",
    "\n",
    "# add batch dimension to match image_batch [1, 3, H, W]\n",
    "mask = mask.unsqueeze(0).to(device)\n",
    "\n",
    "final_mask = mask\n",
    "final_mask = 1 - mask\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(tensor2img(final_mask, denorm=False))\n",
    "plt.axis('off')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9479cd-1540-4658-a629-33fd60a352d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "epochs = 10\n",
    "\n",
    "adv_x = image_batch.clone()\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    iter_grad = get_gradient(adv_x, hard_target_idx)     \n",
    "\n",
    "    iter_grad = iter_grad * final_mask\n",
    "    \n",
    "    adv_x = adv_x - epsilon * iter_grad\n",
    "\n",
    "display_attack_result(adv_x, image_batch, f\"Masked Attack ({epochs} epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5589f2-1ace-4df4-95fb-0442bd25b43d",
   "metadata": {},
   "source": [
    "## Conclusion & Key Takeaways\n",
    "\n",
    "In this laboratory, we explored the fragility of modern Convolutional Neural Networks (CNNs). Despite achieving high accuracy on the ImageNet dataset, we demonstrated that **MobileNetV2** is highly susceptible to **adversarial examples**.\n",
    "\n",
    "Through our experiments, we observed three critical phenomena:\n",
    "\n",
    "1.  **Imperceptible noise:** By calculating the gradient of the loss with respect to the input image ($\\nabla_x J$), we created perturbations that are nearly invisible to the human eye but catastrophic for the model. The **Fast Gradient Sign Method (FGSM)** proved that models often behave linearly in high-dimensional spaces, allowing simple attacks to succeed easily.\n",
    "2.  **Control over predictions:** We moved beyond simple errors to **Targeted Attacks**. By optimizing the input to minimize the loss for a specific target class, we forced the model to hallucinate specific objects (e.g., turning a cat into a specific dog breed or inanimate object), even if the visual semantics did not change for a human observer.\n",
    "3.  **Contextual Bias:** In the final task, we saw that the model does not look at objects in isolation. By masking the attack, we learned that changing only the **background** or only the **foreground** can sometimes be enough to fool the network, revealing that CNNs often rely on contextual correlations rather than just object features.\n",
    "\n",
    "### Food for Thought: Defense\n",
    "\n",
    "If an attacker can manipulate a self-driving car's vision or a medical diagnosis system using invisible noise, how do we trust these systems?\n",
    "\n",
    "This field of study leads naturally to **Adversarial Defense**, which involves:\n",
    "* **Adversarial Training:** Including adversarial examples in the training set so the model learns to ignore small perturbations.\n",
    "* **Gradient masking:** Techniques to hide the gradient information from attackers.\n",
    "* **Robust architectures:** Designing networks that are mathematically guaranteed to be stable within a certain radius $\\epsilon$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
